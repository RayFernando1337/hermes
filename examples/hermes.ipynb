{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3-WuuG6CIFy"
      },
      "source": [
        "# Hermes: Lightning-Fast Video Transcription Tutorial\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Welcome to this tutorial on Hermes, a powerful Python library and CLI tool for lightning-fast video transcription! Developed by [@unclecode](https://twitter.com/unclecode) and powered by cutting-edge AI, Hermes leverages the speed of Groq and the flexibility of multiple providers (Groq, MLX Whisper, and OpenAI) to convert your videos into text.\n",
        "\n",
        "Before we dive in, head over to the GitHub repo and show your support:\n",
        "\n",
        "- **Star the repo:** https://github.com/unclecode/hermes\n",
        "- **Follow me on X:** [@unclecode](https://twitter.com/unclecode)"
      ],
      "id": "e3-WuuG6CIFy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIrtWBTDCIF0"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Let's get Hermes installed! Use pip to install directly from GitHub:"
      ],
      "id": "AIrtWBTDCIF0"
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "R0B4cL2MG5iL",
        "outputId": "ad2515b8-2ee9-423e-8394-03f62f6671b6"
      },
      "id": "R0B4cL2MG5iL",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libasound2-dev is already the newest version (1.2.6.1-1ubuntu1).\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "Suggested packages:\n",
            "  portaudio19-doc\n",
            "The following NEW packages will be installed:\n",
            "  libportaudio2 libportaudiocpp0 portaudio19-dev\n",
            "0 upgraded, 3 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 188 kB of archives.\n",
            "After this operation, 927 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudio2 amd64 19.6.0-1.1 [65.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudiocpp0 amd64 19.6.0-1.1 [16.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 portaudio19-dev amd64 19.6.0-1.1 [106 kB]\n",
            "Fetched 188 kB in 0s (578 kB/s)\n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 123595 files and directories currently installed.)\n",
            "Preparing to unpack .../libportaudio2_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package libportaudiocpp0:amd64.\n",
            "Preparing to unpack .../libportaudiocpp0_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudiocpp0:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package portaudio19-dev:amd64.\n",
            "Preparing to unpack .../portaudio19-dev_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking portaudio19-dev:amd64 (19.6.0-1.1) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Setting up libportaudiocpp0:amd64 (19.6.0-1.1) ...\n",
            "Setting up portaudio19-dev:amd64 (19.6.0-1.1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xoAvO9jRCIF0",
        "outputId": "1fe87c08-620c-478f-aaa5-fb003c930197"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/unclecode/hermes.git@main\n",
            "  Cloning https://github.com/unclecode/hermes.git (to revision main) to /tmp/pip-req-build-xqallhm5\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/unclecode/hermes.git /tmp/pip-req-build-xqallhm5\n",
            "  Resolved https://github.com/unclecode/hermes.git to commit 1dde137d1f7b0c1eefab8d76353c68e1fe36b31b\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting yt-dlp>=2024.8.6 (from hermes==0.1.0)\n",
            "  Downloading yt_dlp-2024.8.6-py3-none-any.whl.metadata (170 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.1/170.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpeg-python>=0.2.0 (from hermes==0.1.0)\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting openai>=1.42.0 (from hermes==0.1.0)\n",
            "  Downloading openai-1.42.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting groq>=0.9.0 (from hermes==0.1.0)\n",
            "  Downloading groq-0.9.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pydub>=0.25.1 (from hermes==0.1.0)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: pyperclip>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from hermes==0.1.0) (1.9.0)\n",
            "Collecting sounddevice>=0.5.0 (from hermes==0.1.0)\n",
            "  Downloading sounddevice-0.5.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting numpy>=2.0.1 (from hermes==0.1.0)\n",
            "  Downloading numpy-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.32.3 in /usr/local/lib/python3.10/dist-packages (from hermes==0.1.0) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.2 in /usr/local/lib/python3.10/dist-packages (from hermes==0.1.0) (6.0.2)\n",
            "Collecting litellm>=1.44.5 (from hermes==0.1.0)\n",
            "  Downloading litellm-1.44.5-py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from ffmpeg-python>=0.2.0->hermes==0.1.0) (1.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq>=0.9.0->hermes==0.1.0) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq>=0.9.0->hermes==0.1.0) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from groq>=0.9.0->hermes==0.1.0)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq>=0.9.0->hermes==0.1.0) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq>=0.9.0->hermes==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq>=0.9.0->hermes==0.1.0) (4.12.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from litellm>=1.44.5->hermes==0.1.0) (3.10.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from litellm>=1.44.5->hermes==0.1.0) (8.1.7)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.10/dist-packages (from litellm>=1.44.5->hermes==0.1.0) (8.4.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from litellm>=1.44.5->hermes==0.1.0) (3.1.4)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from litellm>=1.44.5->hermes==0.1.0) (4.23.0)\n",
            "Collecting python-dotenv>=0.2.0 (from litellm>=1.44.5->hermes==0.1.0)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting tiktoken>=0.7.0 (from litellm>=1.44.5->hermes==0.1.0)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from litellm>=1.44.5->hermes==0.1.0) (0.19.1)\n",
            "Collecting jiter<1,>=0.4.0 (from openai>=1.42.0->hermes==0.1.0)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>=1.42.0->hermes==0.1.0) (4.66.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.3->hermes==0.1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.3->hermes==0.1.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.3->hermes==0.1.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.3->hermes==0.1.0) (2024.7.4)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.5.0->hermes==0.1.0) (1.17.0)\n",
            "Collecting brotli (from yt-dlp>=2024.8.6->hermes==0.1.0)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting mutagen (from yt-dlp>=2024.8.6->hermes==0.1.0)\n",
            "  Downloading mutagen-1.47.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting pycryptodomex (from yt-dlp>=2024.8.6->hermes==0.1.0)\n",
            "  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting websockets>=12.0 (from yt-dlp>=2024.8.6->hermes==0.1.0)\n",
            "  Downloading websockets-13.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq>=0.9.0->hermes==0.1.0) (1.2.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.5.0->hermes==0.1.0) (2.22)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq>=0.9.0->hermes==0.1.0)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq>=0.9.0->hermes==0.1.0)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.8.0->litellm>=1.44.5->hermes==0.1.0) (3.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.44.5->hermes==0.1.0) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.44.5->hermes==0.1.0) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.44.5->hermes==0.1.0) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.44.5->hermes==0.1.0) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.44.5->hermes==0.1.0) (0.20.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq>=0.9.0->hermes==0.1.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq>=0.9.0->hermes==0.1.0) (2.20.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken>=0.7.0->litellm>=1.44.5->hermes==0.1.0) (2024.5.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm>=1.44.5->hermes==0.1.0) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm>=1.44.5->hermes==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm>=1.44.5->hermes==0.1.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm>=1.44.5->hermes==0.1.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm>=1.44.5->hermes==0.1.0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->litellm>=1.44.5->hermes==0.1.0) (4.0.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers->litellm>=1.44.5->hermes==0.1.0) (0.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.44.5->hermes==0.1.0) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.44.5->hermes==0.1.0) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.44.5->hermes==0.1.0) (24.1)\n",
            "Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Downloading groq-0.9.0-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading litellm-1.44.5-py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.42.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.9/362.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading sounddevice-0.5.0-py3-none-any.whl (32 kB)\n",
            "Downloading yt_dlp-2024.8.6-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-13.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (157 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.2/157.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mutagen-1.47.0-py3-none-any.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: hermes\n",
            "  Building wheel for hermes (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hermes: filename=hermes-0.1.0-py3-none-any.whl size=21951 sha256=15f35509dabd77bc8aa8460fe4f2f5ea20ed6a7c983a9bf2d5ca301b3d1eff0f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xuakz5yd/wheels/00/0d/06/7e37737e241322e19a0a573c61b2a89008e212837bd3c730a1\n",
            "Successfully built hermes\n",
            "Installing collected packages: pydub, brotli, websockets, python-dotenv, pycryptodomex, numpy, mutagen, jiter, h11, ffmpeg-python, yt-dlp, tiktoken, sounddevice, httpcore, httpx, openai, groq, litellm, hermes\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xgboost 2.1.1 requires nvidia-nccl-cu12; platform_system == \"Linux\" and platform_machine != \"aarch64\", which is not installed.\n",
            "accelerate 0.32.1 requires numpy<2.0.0,>=1.17, but you have numpy 2.1.0 which is incompatible.\n",
            "albucore 0.0.13 requires numpy<2,>=1.24.4, but you have numpy 2.1.0 which is incompatible.\n",
            "arviz 0.18.0 requires numpy<2.0,>=1.23.0, but you have numpy 2.1.0 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires numpy<2.0a0,>=1.23, but you have numpy 2.1.0 which is incompatible.\n",
            "cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.1.0 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.1.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires numpy<2,>=1, but you have numpy 2.1.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.0 which is incompatible.\n",
            "pandas 2.1.4 requires numpy<2,>=1.22.4; python_version < \"3.11\", but you have numpy 2.1.0 which is incompatible.\n",
            "rmm-cu12 24.4.0 requires numpy<2.0a0,>=1.23, but you have numpy 2.1.0 which is incompatible.\n",
            "scikit-learn 1.3.2 requires numpy<2.0,>=1.17.3, but you have numpy 2.1.0 which is incompatible.\n",
            "tensorflow 2.17.0 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.1.0 which is incompatible.\n",
            "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.1.0 which is incompatible.\n",
            "transformers 4.42.4 requires numpy<2.0,>=1.17, but you have numpy 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed brotli-1.1.0 ffmpeg-python-0.2.0 groq-0.9.0 h11-0.14.0 hermes-0.1.0 httpcore-1.0.5 httpx-0.27.0 jiter-0.5.0 litellm-1.44.5 mutagen-1.47.0 numpy-2.1.0 openai-1.42.0 pycryptodomex-3.20.0 pydub-0.25.1 python-dotenv-1.0.1 sounddevice-0.5.0 tiktoken-0.7.0 websockets-13.0 yt-dlp-2024.8.6\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/unclecode/hermes.git@main"
      ],
      "id": "xoAvO9jRCIF0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BNFpK6ZCIF1"
      },
      "source": [
        "## Python Library"
      ],
      "id": "8BNFpK6ZCIF1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_u-xdquCIF1"
      },
      "source": [
        "### Basic Transcription\n",
        "\n",
        "Here's how to transcribe a local video file using the `transcribe` function with Groq as the provider:"
      ],
      "id": "F_u-xdquCIF1"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4OM8q8RCIF1",
        "outputId": "4cd04ee5-43f0-4a84-df6c-54a85826d4c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Hello, your beautiful people. This is Uncle Code and today I'm going to review quickly the Q1-2 function calling ability. This model is really, to me, is very interesting. First of all, it came up with the really good stuff with 72 billion models and you can take a look, check their blog and especially their instruct model. And also it supports the majority of the language in the world, the regions, the major regions. And that is great because this is a large language model sounds like all of us and it's cool and go and play around with it so what I'm what I'm trying to do is trying to challenge a little bit the function calling like similar the thing that I did for other models like the mistral so first they have this nice library the coin agents that led you to create agentic software and applications and also speed up the work with the large language model what you can just install it quickly and then there are different ways that you can work with the model you can maybe use Olama or Lama CCP and then you download it and install it or I just choose open rotor because they do have this carbon to 72 billion in Strug model and I took it from re-increting account so you can have an API tokens is not difficult anyway I just the code is here from their own GitHub if you want to use by Ollama then then okay then if if you You know what? When you call this models, in a typical way is you call a model, you pass the user query, and then the model returns you back a message that it has function called key in it, and content is empty or is not. And then you take the name of the function and then go to your functions and pass the detected, extract your arguments, get the result back, send it back to the model, and then the model produced the response for you. This process, when you do have multiple calls, a user asking you questions that then when we get the first response then we have another call and again another call so this is going to be in kind of iteration and then you have to continue doing that so what if down I created a helper function which get the LM instance in this example for the coin and then messages in the list of functions you know JSON schema And it goes through them and then there a loop and if the last message of the response from the LLM has function called, and get it out, they call the functions, and create the function message and send it back to the message in each story, and then iterate, till the point that there's no any kind of function call and return it back. So it's a great helper because you can see, you're gonna see it, because one of the thing that I want to try is this multiple call back to each other's. So that's one thing. And also I created some dummy functions. Get current weather, just slip a little bit, and then return back in JSON in always Celsius. This is get current temperature, which is very similar to this one. Doesn't have the format. I just wanted to have multiple version of it. You will see later. And get current time, convert temperature. And I do have some mathematical functions, sine, cosine, exponential, and power. And they're really calculating. If you remember, if you have seen my video about Mistrol, there I challenge it with the complicated mathematical expressions. I wanted to see if you can break it down and then doing it. It was, couldn't, you know, but sort of half-half. Now let's see what's going to happen for this model. First of all, this model will return one functions. So it's not a multiple function, the parallel functions. So you have to iterate, like what I said. Okay, the first example. the better luck today in Paris also tell me the time in San Francisco. So there are two questions not necessarily relevant to each order. You can do this first or you can do the other one. But I want to see how it goes. And I pass those two functions here and I call the execute function. And that's what we're going to see. When you call this, the first things come out of the model is the get current weather in the Paris and location. And this is function result. I should say function called function response, but better than make a function response. So my function, that executed help for function, executed the relevant functions, and got the dummy data, and then pass it to model, and now we have another function called from the model. Model asking for another function to be called is get current time, which is the look values of San Francisco, and then the dummy functions returns back to 10, and then we said the water in Paris in today is 10 degrees Celsius and current time in San Francisco is 10 because I just returned 10 That interesting So the model could really check each one of these questions separately It didn miss stuff between Paris and San Francisco And some weaker models, they do that. Perfect. And I tried, I think you'd be more than the two, three, four. You can continue doing that. That is really cool stuff. And then the second example is a little bit different. So I do have this message first. What's the word like today in Paris? So you can see that the second one is dependent on the first one. So there's a chain between these two, nested calls, binding, and there's dependency actually. So I pass the two functions, get current temperatures and convert temperature. And then let's see what's going to happen. So the first function called, the first function detected is the current temperature, is the Paris. So we execute the function, return it back to the model, and model it's rate again. And the second is the convert temperature, which is the 10 from the previous function called, it and then from Celsius to Fahrenheit and because it's a dummy function as it turns back at 10 always and we got this one perfect so this one Matt that that's really interesting very clean and then you can continue doing that to get the response and and you know by the way you can you can do some apply some smart trick to build your own a parallel function call I'll talk about it maybe someday even even the model doesn't support now Now let's go to the challenge that none of them could do it with the GPD. So I have sine, I have cosine, exponential in the power, and I'm passing this one. Calculate the result of power of sign of exponential 10 plus cosine of the power of two, exponent and then when you took it all, then there's another power, use this as a base and the four as exponent. Okay, so let's see what's going to happen. it here so we have a bunch of function called first of all fantastic that it didn't try to solve it by itself some of the models they do that so it took it really brick it down I like that first went for exponential 10 and that's what you got here then called the sign over the exponential 10 so that is great cool things then went for the power two three perfect then calculate the cosine of that nice eight and then got this and then got the power okay so what it did it did the addition by itself it did the addition between this part and this part let me see if the addition is correct minus this addition doesn't look correct it looks like it looks like subtraction i guess let me test it with my calculator uh is is 0.68 minus yeah it's Yeah, it's subtracted. Basically, this, if this minus this, it's going to give you the, but let me make sure everything is correct. Yeah. Well, okay, but that's beautiful. It got the exponent four, and this is great. This is fantastic. I mean, I think by maybe a little bit of adjustment, we can make it better. Or maybe we can create an extra function sum and don't let the model to do this edition by itself. We make it the sum like a binary operator, this sum of this, comma, this. You get what I mean? I will try that one. But that's fantastic. This absolutely, this is beautiful. This is great. Now I have, when I tried nostril, I said, I think so far this is the best function called model. But now I'm going to tell the same thing for this quote one. But seven to billion parameters, you know, that's the things. I like to have a function calling with 100 million parameters model. Anyway, this is great. This is a good model. I'm going to dig in more into this. And then that's it. I'll share the links to collapse so you can play around whether you can use that helper function. See how it goes. So this was Cohen 2. You can go to their blog. Definitely go to their blogs. A bunch of really interesting information over there. You can see what they have done. And that's it. So, Cohen 2, function calling. It was nice. And Uncle Code out.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['GROQ_API_KEY'] = userdata.get('GROQ_API_KEY')\n",
        "from hermes import transcribe\n",
        "\n",
        "# Replace with the actual path to your video file\n",
        "video_file = 'input.mp4'\n",
        "\n",
        "result = transcribe(video_file, provider='groq')\n",
        "\n",
        "# Print the transcription\n",
        "print(result['transcription'])"
      ],
      "id": "D4OM8q8RCIF1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTZPOUVGCIF1"
      },
      "source": [
        "**Explanation:**\n",
        "\n",
        "- We import the `transcribe` function from the `hermes` library.\n",
        "- We provide the path to our video file.\n",
        "- We specify `provider='groq'` to use Groq's powerful transcription models.\n",
        "- The `transcribe` function returns a dictionary containing the transcription and other metadata."
      ],
      "id": "aTZPOUVGCIF1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0mPy3xrCIF2"
      },
      "source": [
        "### Transcribing YouTube Videos\n",
        "\n",
        "Transcribing YouTube videos is a breeze with Hermes. Simply pass the YouTube URL to the `transcribe` function:"
      ],
      "id": "O0mPy3xrCIF2"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYyESkhMCIF2",
        "outputId": "f26119ea-13a3-4829-e1a4-db5dcc3fe154"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=dQw4w9WgXcQ\n",
            "[youtube] dQw4w9WgXcQ: Downloading webpage\n",
            "[youtube] dQw4w9WgXcQ: Downloading ios player API JSON\n",
            "[youtube] dQw4w9WgXcQ: Downloading web creator player API JSON\n",
            "[youtube] dQw4w9WgXcQ: Downloading player a87a9450\n",
            "[youtube] dQw4w9WgXcQ: Downloading m3u8 information\n",
            "[info] dQw4w9WgXcQ: Downloading 1 format(s): 251\n",
            "[download] Destination: dQw4w9WgXcQ.webm\n",
            "[download] 100% of    3.28MiB in 00:00:00 at 10.12MiB/s  \n",
            "[ExtractAudio] Destination: dQw4w9WgXcQ.mp3\n",
            "Deleting original file dQw4w9WgXcQ.webm (pass -k to keep)\n",
            " We're no strange to love. You know the rules, and so do I. I feel commitments want to thinking us. You wouldn't get this from any other guy I just want to tell you how I'm feeling Gotta make you understand Never gonna give you up Never gonna let you down Never gonna run around and desert you Never gonna make you cry Never gonna say goodbye Never gonna tell the lie And hurt you We've known each other for so long. Your heart's been aching, but you're too shy to say it. Inside we both know what's been going on. We know the game and we're gonna play it. And if you ask me how I'm feeling, don't tell me you're too blind to see. Never gonna give you up, never gonna let you down. down never gonna run around and desert you never gonna make you cry never gonna say goodbye never gonna tell a lie and hurt you never gonna give you up never gonna let you down never gonna run around and desert you never gonna make you cry never gonna say goodbye never gonna tell a bye and hurt you Never gonna give Never gonna give We've known each other For so long Your heart's been aching but You're too shy to say it It's how we both know what's been going on We know the game and we're gonna play it I just want to tell you how I feel like Gotta make you understand Never gonna give you up, never gonna let you down, never gonna run around and desert you, never gonna make you cry, never gonna say goodbye, never gonna tell a lie, and hurt you, never gonna give you up, never gonna let you down, never gonna run around and desert you, never gonna make you cry, never gonna say good bye, never gonna say good, never gonna say good Bye. Never going to say goodbye and hurt you. Never going to give you up. Never going to let you down. Never going to run around and desert you. Never going to make you cry. Never going to say goodbye. That I'm going to celebrate.\n"
          ]
        }
      ],
      "source": [
        "from hermes import transcribe\n",
        "\n",
        "youtube_url = 'https://www.youtube.com/watch?v=PNulbFECY-I'  # Example URL\n",
        "\n",
        "result = transcribe(youtube_url, provider='groq')\n",
        "print(result['transcription'])"
      ],
      "id": "TYyESkhMCIF2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-Hky9-oCIF2"
      },
      "source": [
        "**Explanation:**\n",
        "\n",
        "- Hermes handles the YouTube video download automatically.\n",
        "- No need to manually download the video!"
      ],
      "id": "K-Hky9-oCIF2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfGkV9jNCIF2"
      },
      "source": [
        "### Using Different Models\n",
        "\n",
        "Hermes supports various transcription models. You can specify the desired model using the `model` parameter:"
      ],
      "id": "YfGkV9jNCIF2"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQZH3e80CIF2",
        "outputId": "b8a9d956-b597-4a09-b374-c7556cfea79a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Hello, you beautiful people. This is Uncle Code. And today I'm going to review quickly the Q1.2 function calling ability. This model is really, to me, is very interesting. First of all, it came up with a really good stuff with 72 billion models. And you can take a look, check their blog and especially their instruct model. And also it supports the majority of the language in the world, the regions, the major regions, and that is great because this is a large language model. sounds like all of us. And it's cool, and go and play around with it. So what I'm trying to do is trying to challenge a little bit the function calling. Like similar to the thing that I did for other models like the Mistral. So first, they have this nice library, the Cohen agents, that let you to create agent-next softwares and applications and also speed up the work with the large language model. What you can just install it quickly. And then there are different ways that you can work with the model. You can maybe use a Lama or Lama CCP and then you download it and install it. Or I just choose OpenRotor because they do have this going to 72 billion in stroke model. And I took it from reincred in accounts. You can have an API tokens, it's not difficult. Anyway, I just, the code is here from their own GitHub. If you want to use by Ollama. Then, okay, then if, you know what, when you call this models in typical way is you call a model, you pass the user query, and then the model returns you back a message that it has function called key in it, and content is empty or is not. And then you take the name of the function and then go to your functions and pass the detected, extracted arguments, get the result back, send it back to the model, and then the model produce the response for you, right? This process, when you do have multiple calls, let's say user asking you questions, that then when we get the first response, then we have another call, and again another call. So this is gonna be in kind of iteration. And then you have to continue doing that. So what I done I created a helper function which get the LME stance in this example for the coin and then messages in the list of functions you know JSON schema And it goes through them and then there a loop and if the last message of the response from the LLM has function call, it get it out, it call the functions and create the function message and send it back to the messaging story, and then iterate to the point that there's no any kind of function call and return it back. So it's a great helper because you can see, you're going to see, because one of the things I want to try is this multiple call back to each other. So that's one thing. And also I created some dummy functions, get current weather, it just slip a little bit and then return back in JSON in always Celsius. This is get current temperature, which is very similar to this one. It doesn't have the format. I just wanted to have multiple version of it. You will see later. And get current time, convert temperature. And I do have some mathematical functions, sine, cosine, exponential, and power and they're really calculating if you remember if you have seen my video about mistral there i challenge it with the complicated mathematical expressions i wanted to see if can break it down and then doing it it was couldn't you know but sort of half half now let's see what's going to happen for this model first of all this model always returns one functions so it's not a multiple function the parallel functions so you have to each way like what i said okay the first example what's the better luck today in paris also tell me the time in san francisco so there are two questions not necessarily relevant to each order so you can do this first or you can do the other one but i want to see how it goes and i pass those two functions here and i call the execute function and that's what we're going to see when you call this the first things come out of the model is the get current weather in the Paris and location and this is function result I should say function called function response but better than making function response so so my function that execute help for function executed the relevant functions and got the dummy data and then pass it to model and now we have another function called from the model model asking for the function to be called is get parent time which is the look values of San Francisco and then the dummy functions returns back to 10 and then we said the water in Paris in today is 10 degrees Celsius and current time in San Francisco is 10 because I just returned 10 That interesting So the model could really check each one of these questions separately It didn mess up between Paris and San Francisco. And some weaker models, they do that. Perfect. And I tried, I think you'd be more than the two, three, four. You can continue doing that. That is really cool stuff. And then the second example is a little bit different. So I do have this message first. What's the water like today in Paris, also converted to Fahrenheit. So you can see that the second one is dependent on the first one. So there's a chain between these two, nested calls, binding, and there's dependency actually. So I pass the two functions, get current temperatures and convert temperature, and then let's see what's going to happen. So first function call, the first function detected is the current temperature is the Paris. So we execute the function, return it back to the model, and model iterate again. And the second is the convert temperature, which is the 10 from the previous function call, took it, and then from Celsius to Fahrenheit, and because it's a dummy function, it returns back at 10 always, and we got this one. Perfect. So this one, that's really interesting, very clean, and then you can continue doing that to get the response. And you know, by the way, you can apply some smart trick to build your own parallel function call. I'll talk about it maybe someday even even the model doesn't support now let's go to the challenge that none of them could do it even invented gpd so i have sine i have cosine exponential in the power and i'm passing this one calculate the result of power of sine of exponential 10 plus cosine of the power of 2 exponent in 3 and then when you took it all then there's another power use this as a base and the for as exponent okay so let's see what's going to happen execute it here so we have a bunch of function calls first of all fantastic that it didn't try to solve it by itself some of the models they do that so it took it really break it down i like that first went for exponential 10 and and that what it got here then called the sign over the exponential 10 so that is great cool things then went for the power two three perfect then calculate the cosine of that nice eight and then got this and then got the power okay so what it did it did the addition by itself it did the addition between this part and this part let me see if the addition is correct minus this the addition doesn't looks correct it looks like it looks like subtraction i guess let me test it with my calculator uh is is 0.68 minus yeah it's yeah it's it's subtracted basically this if if this minus this it going to give you the but let me make sure everything is correct yeah well okay but that's beautiful it got the exponent four and and this is great this is fantastic i mean i think by maybe a little bit of adjustment we can make it better or maybe we can create an extra function sum and don't let the model to do this addition by itself we make it the sum like a binary operator There's this sum of this comma this. You get what I mean? I will try that one. But that's just fantastic. Absolutely, this is beautiful. This is great. Now, when I tried NISTROL, I said, I think so far this is the best function call model. But now I'm going to tell the same thing for this code. But 72 billion parameters. You know, that's the things. I like to have a function calling with 100 million parameters model. anyway um this is great this is a good model i i'm gonna dig in more into this and then that's it uh i'll share the links to colab so you can play around with it you can use that helper function see how it goes so this was cohen2 you can go to their blog definitely go to their blogs and read that a bunch of really interesting information over there you can see what they have done and that's it so cohen 2 function calling it was nice and uncle code out\n"
          ]
        }
      ],
      "source": [
        "from hermes import transcribe\n",
        "\n",
        "video_file = 'input.mp4'\n",
        "\n",
        "result = transcribe(video_file, provider='groq', model='whisper-large-v3')\n",
        "print(result['transcription'])"
      ],
      "id": "zQZH3e80CIF2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAuGXpr_CIF2"
      },
      "source": [
        "**Explanation:**\n",
        "\n",
        "- Here, we use the `whisper-large-v3` model instead of the default `distil-whisper` model."
      ],
      "id": "iAuGXpr_CIF2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn0HQ0vICIF2"
      },
      "source": [
        "### JSON Output and LLM Processing\n",
        "\n",
        "- To get the transcription in JSON format, set `response_format='json'`.\n",
        "- To further process the transcription with an LLM (e.g., for summarization), use the `llm_prompt` parameter:"
      ],
      "id": "dn0HQ0vICIF2"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RK077hZiCIF2",
        "outputId": "05c9ae86-9aea-4e8c-c391-48f747af14d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"text\":\" Hello, your beautiful people. This is Uncle Code and today I'm going to review quickly the Q1-2 function calling ability. This model is really, to me, is very interesting. First of all, it came up with the really good stuff with 72 billion models and you can take a look, check their blog and especially their instruct model. And also it supports the majority of the language in the world, the regions, the major regions. And that is great because this is a large language model sounds like all of us and it's cool and go and play around with it so what I'm what I'm trying to do is trying to challenge a little bit the function calling like similar the thing that I did for other models like the mistral so first they have this nice library the coin agents that led you to create agentic software and applications and also speed up the work with the large language model what you can just install it quickly and then there are different ways that you can work with the model you can maybe use Olama or Lama CCP and then you download it and install it or I just choose open rotor because they do have this carbon to 72 billion in Strug model and I took it from re-increting account so you can have an API tokens is not difficult anyway I just the code is here from their own GitHub if you want to use by Ollama then then okay then if if you You know what? When you call this models, in a typical way is you call a model, you pass the user query, and then the model returns you back a message that it has function called key in it, and content is empty or is not. And then you take the name of the function and then go to your functions and pass the detected, extract your arguments, get the result back, send it back to the model, and then the model produced the response for you. This process, when you do have multiple calls, a user asking you questions that then when we get the first response then we have another call and again another call so this is going to be in kind of iteration and then you have to continue doing that so what if down I created a helper function which get the LM instance in this example for the coin and then messages in the list of functions you know JSON schema And it goes through them and then there a loop and if the last message of the response from the LLM has function called, and get it out, they call the functions, and create the function message and send it back to the message in each story, and then iterate, till the point that there's no any kind of function call and return it back. So it's a great helper because you can see, you're gonna see it, because one of the thing that I want to try is this multiple call back to each other's. So that's one thing. And also I created some dummy functions. Get current weather, just slip a little bit, and then return back in JSON in always Celsius. This is get current temperature, which is very similar to this one. Doesn't have the format. I just wanted to have multiple version of it. You will see later. And get current time, convert temperature. And I do have some mathematical functions, sine, cosine, exponential, and power. And they're really calculating. If you remember, if you have seen my video about Mistrol, there I challenge it with the complicated mathematical expressions. I wanted to see if you can break it down and then doing it. It was, couldn't, you know, but sort of half-half. Now let's see what's going to happen for this model. First of all, this model will return one functions. So it's not a multiple function, the parallel functions. So you have to iterate, like what I said. Okay, the first example. the better luck today in Paris also tell me the time in San Francisco. So there are two questions not necessarily relevant to each order. You can do this first or you can do the other one. But I want to see how it goes. And I pass those two functions here and I call the execute function. And that's what we're going to see. When you call this, the first things come out of the model is the get current weather in the Paris and location. And this is function result. I should say function called function response, but better than make a function response. So my function, that executed help for function, executed the relevant functions, and got the dummy data, and then pass it to model, and now we have another function called from the model. Model asking for another function to be called is get current time, which is the look values of San Francisco, and then the dummy functions returns back to 10, and then we said the water in Paris in today is 10 degrees Celsius and current time in San Francisco is 10 because I just returned 10 That interesting So the model could really check each one of these questions separately It didn miss stuff between Paris and San Francisco And some weaker models, they do that. Perfect. And I tried, I think you'd be more than the two, three, four. You can continue doing that. That is really cool stuff. And then the second example is a little bit different. So I do have this message first. What's the word like today in Paris? So you can see that the second one is dependent on the first one. So there's a chain between these two, nested calls, binding, and there's dependency actually. So I pass the two functions, get current temperatures and convert temperature. And then let's see what's going to happen. So the first function called, the first function detected is the current temperature, is the Paris. So we execute the function, return it back to the model, and model it's rate again. And the second is the convert temperature, which is the 10 from the previous function called, it and then from Celsius to Fahrenheit and because it's a dummy function as it turns back at 10 always and we got this one perfect so this one Matt that that's really interesting very clean and then you can continue doing that to get the response and and you know by the way you can you can do some apply some smart trick to build your own a parallel function call I'll talk about it maybe someday even even the model doesn't support now Now let's go to the challenge that none of them could do it with the GPD. So I have sine, I have cosine, exponential in the power, and I'm passing this one. Calculate the result of power of sign of exponential 10 plus cosine of the power of two, exponent and then when you took it all, then there's another power, use this as a base and the four as exponent. Okay, so let's see what's going to happen. it here so we have a bunch of function called first of all fantastic that it didn't try to solve it by itself some of the models they do that so it took it really brick it down I like that first went for exponential 10 and that's what you got here then called the sign over the exponential 10 so that is great cool things then went for the power two three perfect then calculate the cosine of that nice eight and then got this and then got the power okay so what it did it did the addition by itself it did the addition between this part and this part let me see if the addition is correct minus this addition doesn't look correct it looks like it looks like subtraction i guess let me test it with my calculator uh is is 0.68 minus yeah it's Yeah, it's subtracted. Basically, this, if this minus this, it's going to give you the, but let me make sure everything is correct. Yeah. Well, okay, but that's beautiful. It got the exponent four, and this is great. This is fantastic. I mean, I think by maybe a little bit of adjustment, we can make it better. Or maybe we can create an extra function sum and don't let the model to do this edition by itself. We make it the sum like a binary operator, this sum of this, comma, this. You get what I mean? I will try that one. But that's fantastic. This absolutely, this is beautiful. This is great. Now I have, when I tried nostril, I said, I think so far this is the best function called model. But now I'm going to tell the same thing for this quote one. But seven to billion parameters, you know, that's the things. I like to have a function calling with 100 million parameters model. Anyway, this is great. This is a good model. I'm going to dig in more into this. And then that's it. I'll share the links to collapse so you can play around whether you can use that helper function. See how it goes. So this was Cohen 2. You can go to their blog. Definitely go to their blogs. A bunch of really interesting information over there. You can see what they have done. And that's it. So, Cohen 2, function calling. It was nice. And Uncle Code out.\",\"x_groq\":{\"id\":\"req_01j67j2k3bf4br65txs4fnzpf3\"}}\n",
            "\n",
            "Here are 3 bullet points summarizing the transcription:\n",
            "\n",
            "* The speaker, Uncle Code, reviews the Q1-2 function calling ability of a large language model and finds it impressive, comparing it to its previous experiments with other models. He highlights that the model has 72 billion parameters and supports multiple languages and regions.\n",
            "* Uncle Code creates a helper function to simplify function calling with the model by passing a list of functions, which are then iteratively called and executed by the model, resulting in a cascading response. He demonstrates this by passing multiple functions and shows the model's ability to handle nested calls and dependencies.\n",
            "* The speaker also conducts an experiment where he challenges the model with a complex mathematical expression involving trigonometric and exponential functions. To his surprise, the model breaks down the expression into smaller, executable components, unlike other models that may try to solve it directly or fail. Uncle Code is impressed with the model's ability to handle function calling and suggests potential improvements to optimize the process further.\n"
          ]
        }
      ],
      "source": [
        "from hermes import transcribe\n",
        "\n",
        "video_file = 'input.mp4'\n",
        "\n",
        "# Get JSON output\n",
        "result = transcribe(video_file, provider='groq', response_format='json')\n",
        "print(result['transcription'])\n",
        "\n",
        "# Summarize with LLM\n",
        "result = transcribe(video_file, provider='groq', llm_prompt=\"Summarize this transcription in 3 bullet points\")\n",
        "print(result['llm_processed'])"
      ],
      "id": "RK077hZiCIF2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Gi2p6ONCIF2"
      },
      "source": [
        "**Explanation:**\n",
        "\n",
        "-  LLM processing requires an API key for the LLM provider (e.g., Groq). Make sure to set it up in your `~/.hermes/config.yml` file or as an environment variable."
      ],
      "id": "2Gi2p6ONCIF2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K12tLtI7CIF3"
      },
      "source": [
        "## Command Line Interface (CLI)\n",
        "\n",
        "Hermes also provides a convenient CLI for transcribing videos. Here are some examples:"
      ],
      "id": "K12tLtI7CIF3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYxTYt86CIF3"
      },
      "source": [
        "### Basic Usage"
      ],
      "id": "IYxTYt86CIF3"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PEQ0tLpCIF3",
        "outputId": "009c2a4e-0e25-4010-b301-708ae9e10744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Hello, your beautiful people. This is Uncle Code and today I'm going to review quickly the Q1-2 function calling ability. This model is really, to me, is very interesting. First of all, it came up with the really good stuff with 72 billion models and you can take a look, check their blog and especially their instruct model. And also it supports the majority of the language in the world, the regions, the major regions. And that is great because this is a large language model sounds like all of us and it's cool and go and play around with it so what I'm what I'm trying to do is trying to challenge a little bit the function calling like similar the thing that I did for other models like the mistral so first they have this nice library the coin agents that led you to create agentic software and applications and also speed up the work with the large language model what you can just install it quickly and then there are different ways that you can work with the model you can maybe use Olama or Lama CCP and then you download it and install it or I just choose open rotor because they do have this carbon to 72 billion in Strug model and I took it from re-increting account so you can have an API tokens is not difficult anyway I just the code is here from their own GitHub if you want to use by Ollama then then okay then if if you You know what? When you call this models, in a typical way is you call a model, you pass the user query, and then the model returns you back a message that it has function called key in it, and content is empty or is not. And then you take the name of the function and then go to your functions and pass the detected, extract your arguments, get the result back, send it back to the model, and then the model produced the response for you. This process, when you do have multiple calls, a user asking you questions that then when we get the first response then we have another call and again another call so this is going to be in kind of iteration and then you have to continue doing that so what if down I created a helper function which get the LM instance in this example for the coin and then messages in the list of functions you know JSON schema And it goes through them and then there a loop and if the last message of the response from the LLM has function called, and get it out, they call the functions, and create the function message and send it back to the message in each story, and then iterate, till the point that there's no any kind of function call and return it back. So it's a great helper because you can see, you're gonna see it, because one of the thing that I want to try is this multiple call back to each other's. So that's one thing. And also I created some dummy functions. Get current weather, just slip a little bit, and then return back in JSON in always Celsius. This is get current temperature, which is very similar to this one. Doesn't have the format. I just wanted to have multiple version of it. You will see later. And get current time, convert temperature. And I do have some mathematical functions, sine, cosine, exponential, and power. And they're really calculating. If you remember, if you have seen my video about Mistrol, there I challenge it with the complicated mathematical expressions. I wanted to see if you can break it down and then doing it. It was, couldn't, you know, but sort of half-half. Now let's see what's going to happen for this model. First of all, this model will return one functions. So it's not a multiple function, the parallel functions. So you have to iterate, like what I said. Okay, the first example. the better luck today in Paris also tell me the time in San Francisco. So there are two questions not necessarily relevant to each order. You can do this first or you can do the other one. But I want to see how it goes. And I pass those two functions here and I call the execute function. And that's what we're going to see. When you call this, the first things come out of the model is the get current weather in the Paris and location. And this is function result. I should say function called function response, but better than make a function response. So my function, that executed help for function, executed the relevant functions, and got the dummy data, and then pass it to model, and now we have another function called from the model. Model asking for another function to be called is get current time, which is the look values of San Francisco, and then the dummy functions returns back to 10, and then we said the water in Paris in today is 10 degrees Celsius and current time in San Francisco is 10 because I just returned 10 That interesting So the model could really check each one of these questions separately It didn miss stuff between Paris and San Francisco And some weaker models, they do that. Perfect. And I tried, I think you'd be more than the two, three, four. You can continue doing that. That is really cool stuff. And then the second example is a little bit different. So I do have this message first. What's the word like today in Paris? So you can see that the second one is dependent on the first one. So there's a chain between these two, nested calls, binding, and there's dependency actually. So I pass the two functions, get current temperatures and convert temperature. And then let's see what's going to happen. So the first function called, the first function detected is the current temperature, is the Paris. So we execute the function, return it back to the model, and model it's rate again. And the second is the convert temperature, which is the 10 from the previous function called, it and then from Celsius to Fahrenheit and because it's a dummy function as it turns back at 10 always and we got this one perfect so this one Matt that that's really interesting very clean and then you can continue doing that to get the response and and you know by the way you can you can do some apply some smart trick to build your own a parallel function call I'll talk about it maybe someday even even the model doesn't support now Now let's go to the challenge that none of them could do it with the GPD. So I have sine, I have cosine, exponential in the power, and I'm passing this one. Calculate the result of power of sign of exponential 10 plus cosine of the power of two, exponent and then when you took it all, then there's another power, use this as a base and the four as exponent. Okay, so let's see what's going to happen. it here so we have a bunch of function called first of all fantastic that it didn't try to solve it by itself some of the models they do that so it took it really brick it down I like that first went for exponential 10 and that's what you got here then called the sign over the exponential 10 so that is great cool things then went for the power two three perfect then calculate the cosine of that nice eight and then got this and then got the power okay so what it did it did the addition by itself it did the addition between this part and this part let me see if the addition is correct minus this addition doesn't look correct it looks like it looks like subtraction i guess let me test it with my calculator uh is is 0.68 minus yeah it's Yeah, it's subtracted. Basically, this, if this minus this, it's going to give you the, but let me make sure everything is correct. Yeah. Well, okay, but that's beautiful. It got the exponent four, and this is great. This is fantastic. I mean, I think by maybe a little bit of adjustment, we can make it better. Or maybe we can create an extra function sum and don't let the model to do this edition by itself. We make it the sum like a binary operator, this sum of this, comma, this. You get what I mean? I will try that one. But that's fantastic. This absolutely, this is beautiful. This is great. Now I have, when I tried nostril, I said, I think so far this is the best function called model. But now I'm going to tell the same thing for this quote one. But seven to billion parameters, you know, that's the things. I like to have a function calling with 100 million parameters model. Anyway, this is great. This is a good model. I'm going to dig in more into this. And then that's it. I'll share the links to collapse so you can play around whether you can use that helper function. See how it goes. So this was Cohen 2. You can go to their blog. Definitely go to their blogs. A bunch of really interesting information over there. You can see what they have done. And that's it. So, Cohen 2, function calling. It was nice. And Uncle Code out.\n"
          ]
        }
      ],
      "source": [
        "!hermes input.mp4 -p groq"
      ],
      "id": "2PEQ0tLpCIF3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-JuXmM0CIF3"
      },
      "source": [
        "### YouTube Videos"
      ],
      "id": "J-JuXmM0CIF3"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJbF3j6bCIF3",
        "outputId": "fb4142a5-18c0-4b28-fc7b-5dd16ed1c636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=PNulbFECY-I\n",
            "[youtube] PNulbFECY-I: Downloading webpage\n",
            "[youtube] PNulbFECY-I: Downloading ios player API JSON\n",
            "[youtube] PNulbFECY-I: Downloading web creator player API JSON\n",
            "[youtube] PNulbFECY-I: Downloading m3u8 information\n",
            "[info] PNulbFECY-I: Downloading 1 format(s): 251\n",
            "[download] Destination: PNulbFECY-I.webm\n",
            "\u001b[K[download] 100% of    2.13MiB in \u001b[1;37m00:00:00\u001b[0m at \u001b[0;32m6.55MiB/s\u001b[0m\n",
            "[ExtractAudio] Destination: PNulbFECY-I.mp3\n",
            "Deleting original file PNulbFECY-I.webm (pass -k to keep)\n",
            " Much is said about the virtues and pleasures of individuality, of being someone who stands out from the crowd and delights in their own particularity. But let's also admit to how frankly lonely and frightening it can be to find ourselves yet again in a peculiar minority where the differences between us and others strike us as bewildering rather than emboldening. When, for example, everyone seems to want to gossip, but we prefer generosity and forgiveness. When everyone is at ease, but we're melancholy and self-conscious. When everyone is cheerful, but we can't seem to let go of anxiety and apprehension. When everyone seems confident, but we feel suspicious and ashamed of ourselves. When everyone is contented in their couples, but we're still searching. for a home, when everyone worries passionately about the future of the planet, but we feel cold and at times almost indifferent, when everyone seems to love life, but we're not sure if we quite do. At such times we might benefit from a few thoughts to alleviate the isolation Firstly we don know reality as well as we imagine What we believe that everyone is like may not be how they actually are We may have more friends than we think Also we getting statistics wrong These four or eight or twelve people in a room don't represent all of humanity. The 80 or so people in our extended social group are in fact always a minuscule part of the human story. There are still so many friends left to meet. Also, perhaps our existing companions actually know much more about the material we feel alone with than we suspect. They and we simply haven't found a way to share our true selves. Maybe they will feel what we feel one day, just not yet. It may be fine to belong to a minority. Minorities have sheltered some of the most accomplished spirits of it ever lived. Lived. Isolation may just be a price we have to pay for a certain complexity of mind. And lastly, we have art to bridge the gaps between ourselves and other people. Bookshops are an ideal destination for the lonely, given how many books were written because their authors couldn't find anyone to talk to. Maybe there are people nearby, perhaps in this community, who would understand who would understand very well indeed.\n"
          ]
        }
      ],
      "source": [
        "!hermes https://www.youtube.com/watch?v=PNulbFECY-I -p groq"
      ],
      "id": "TJbF3j6bCIF3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxauRAa8CIF3"
      },
      "source": [
        "### Different Models"
      ],
      "id": "qxauRAa8CIF3"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCa8loClCIF3",
        "outputId": "a6678b3c-7b4e-48f0-819a-64cd8f4b73db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Hello, you beautiful people. This is Uncle Code. And today I'm going to review quickly the Q1.2 function calling ability. This model is really, to me, is very interesting. First of all, it came up with a really good stuff with 72 billion models. And you can take a look, check their blog and especially their instruct model. And also it supports the majority of the language in the world, the regions, the major regions, and that is great because this is a large language model. sounds like all of us. And it's cool, and go and play around with it. So what I'm trying to do is trying to challenge a little bit the function calling. Like similar to the thing that I did for other models like the Mistral. So first, they have this nice library, the Cohen agents, that let you to create agent-next softwares and applications and also speed up the work with the large language model. What you can just install it quickly. And then there are different ways that you can work with the model. You can maybe use a Lama or Lama CCP and then you download it and install it. Or I just choose OpenRotor because they do have this going to 72 billion in stroke model. And I took it from reincred in accounts. You can have an API tokens, it's not difficult. Anyway, I just, the code is here from their own GitHub. If you want to use by Ollama. Then, okay, then if, you know what, when you call this models in typical way is you call a model, you pass the user query, and then the model returns you back a message that it has function called key in it, and content is empty or is not. And then you take the name of the function and then go to your functions and pass the detected, extracted arguments, get the result back, send it back to the model, and then the model produce the response for you, right? This process, when you do have multiple calls, let's say user asking you questions, that then when we get the first response, then we have another call, and again another call. So this is gonna be in kind of iteration. And then you have to continue doing that. So what I done I created a helper function which get the LME stance in this example for the coin and then messages in the list of functions you know JSON schema And it goes through them and then there a loop and if the last message of the response from the LLM has function call, it get it out, it call the functions and create the function message and send it back to the messaging story, and then iterate to the point that there's no any kind of function call and return it back. So it's a great helper because you can see, you're going to see, because one of the things I want to try is this multiple call back to each other. So that's one thing. And also I created some dummy functions, get current weather, it just slip a little bit and then return back in JSON in always Celsius. This is get current temperature, which is very similar to this one. It doesn't have the format. I just wanted to have multiple version of it. You will see later. And get current time, convert temperature. And I do have some mathematical functions, sine, cosine, exponential, and power and they're really calculating if you remember if you have seen my video about mistral there i challenge it with the complicated mathematical expressions i wanted to see if can break it down and then doing it it was couldn't you know but sort of half half now let's see what's going to happen for this model first of all this model always returns one functions so it's not a multiple function the parallel functions so you have to each way like what i said okay the first example what's the better luck today in paris also tell me the time in san francisco so there are two questions not necessarily relevant to each order so you can do this first or you can do the other one but i want to see how it goes and i pass those two functions here and i call the execute function and that's what we're going to see when you call this the first things come out of the model is the get current weather in the Paris and location and this is function result I should say function called function response but better than making function response so so my function that execute help for function executed the relevant functions and got the dummy data and then pass it to model and now we have another function called from the model model asking for the function to be called is get parent time which is the look values of San Francisco and then the dummy functions returns back to 10 and then we said the water in Paris in today is 10 degrees Celsius and current time in San Francisco is 10 because I just returned 10 That interesting So the model could really check each one of these questions separately It didn mess up between Paris and San Francisco. And some weaker models, they do that. Perfect. And I tried, I think you'd be more than the two, three, four. You can continue doing that. That is really cool stuff. And then the second example is a little bit different. So I do have this message first. What's the water like today in Paris, also converted to Fahrenheit. So you can see that the second one is dependent on the first one. So there's a chain between these two, nested calls, binding, and there's dependency actually. So I pass the two functions, get current temperatures and convert temperature, and then let's see what's going to happen. So first function call, the first function detected is the current temperature is the Paris. So we execute the function, return it back to the model, and model iterate again. And the second is the convert temperature, which is the 10 from the previous function call, took it, and then from Celsius to Fahrenheit, and because it's a dummy function, it returns back at 10 always, and we got this one. Perfect. So this one, that's really interesting, very clean, and then you can continue doing that to get the response. And you know, by the way, you can apply some smart trick to build your own parallel function call. I'll talk about it maybe someday even even the model doesn't support now let's go to the challenge that none of them could do it even invented gpd so i have sine i have cosine exponential in the power and i'm passing this one calculate the result of power of sine of exponential 10 plus cosine of the power of 2 exponent in 3 and then when you took it all then there's another power use this as a base and the for as exponent okay so let's see what's going to happen execute it here so we have a bunch of function calls first of all fantastic that it didn't try to solve it by itself some of the models they do that so it took it really break it down i like that first went for exponential 10 and and that what it got here then called the sign over the exponential 10 so that is great cool things then went for the power two three perfect then calculate the cosine of that nice eight and then got this and then got the power okay so what it did it did the addition by itself it did the addition between this part and this part let me see if the addition is correct minus this the addition doesn't looks correct it looks like it looks like subtraction i guess let me test it with my calculator uh is is 0.68 minus yeah it's yeah it's it's subtracted basically this if if this minus this it going to give you the but let me make sure everything is correct yeah well okay but that's beautiful it got the exponent four and and this is great this is fantastic i mean i think by maybe a little bit of adjustment we can make it better or maybe we can create an extra function sum and don't let the model to do this addition by itself we make it the sum like a binary operator There's this sum of this comma this. You get what I mean? I will try that one. But that's just fantastic. Absolutely, this is beautiful. This is great. Now, when I tried NISTROL, I said, I think so far this is the best function call model. But now I'm going to tell the same thing for this code. But 72 billion parameters. You know, that's the things. I like to have a function calling with 100 million parameters model. anyway um this is great this is a good model i i'm gonna dig in more into this and then that's it uh i'll share the links to colab so you can play around with it you can use that helper function see how it goes so this was cohen2 you can go to their blog definitely go to their blogs and read that a bunch of really interesting information over there you can see what they have done and that's it so cohen 2 function calling it was nice and uncle code out\n"
          ]
        }
      ],
      "source": [
        "!hermes input.mp4 -p groq -m whisper-large-v3"
      ],
      "id": "eCa8loClCIF3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chb_DIGPCIF3"
      },
      "source": [
        "### JSON Output"
      ],
      "id": "chb_DIGPCIF3"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUsIWsbkCIF3",
        "outputId": "4a509de1-74e6-4ab7-f147-f813c504a0c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"text\":\" Hello, your beautiful people. This is Uncle Code and today I'm going to review quickly the Q1-2 function calling ability. This model is really, to me, is very interesting. First of all, it came up with the really good stuff with 72 billion models and you can take a look, check their blog and especially their instruct model. And also it supports the majority of the language in the world, the regions, the major regions. And that is great because this is a large language model sounds like all of us and it's cool and go and play around with it so what I'm what I'm trying to do is trying to challenge a little bit the function calling like similar the thing that I did for other models like the mistral so first they have this nice library the coin agents that led you to create agentic software and applications and also speed up the work with the large language model what you can just install it quickly and then there are different ways that you can work with the model you can maybe use Olama or Lama CCP and then you download it and install it or I just choose open rotor because they do have this carbon to 72 billion in Strug model and I took it from re-increting account so you can have an API tokens is not difficult anyway I just the code is here from their own GitHub if you want to use by Ollama then then okay then if if you You know what? When you call this models, in a typical way is you call a model, you pass the user query, and then the model returns you back a message that it has function called key in it, and content is empty or is not. And then you take the name of the function and then go to your functions and pass the detected, extract your arguments, get the result back, send it back to the model, and then the model produced the response for you. This process, when you do have multiple calls, a user asking you questions that then when we get the first response then we have another call and again another call so this is going to be in kind of iteration and then you have to continue doing that so what if down I created a helper function which get the LM instance in this example for the coin and then messages in the list of functions you know JSON schema And it goes through them and then there a loop and if the last message of the response from the LLM has function called, and get it out, they call the functions, and create the function message and send it back to the message in each story, and then iterate, till the point that there's no any kind of function call and return it back. So it's a great helper because you can see, you're gonna see it, because one of the thing that I want to try is this multiple call back to each other's. So that's one thing. And also I created some dummy functions. Get current weather, just slip a little bit, and then return back in JSON in always Celsius. This is get current temperature, which is very similar to this one. Doesn't have the format. I just wanted to have multiple version of it. You will see later. And get current time, convert temperature. And I do have some mathematical functions, sine, cosine, exponential, and power. And they're really calculating. If you remember, if you have seen my video about Mistrol, there I challenge it with the complicated mathematical expressions. I wanted to see if you can break it down and then doing it. It was, couldn't, you know, but sort of half-half. Now let's see what's going to happen for this model. First of all, this model will return one functions. So it's not a multiple function, the parallel functions. So you have to iterate, like what I said. Okay, the first example. the better luck today in Paris also tell me the time in San Francisco. So there are two questions not necessarily relevant to each order. You can do this first or you can do the other one. But I want to see how it goes. And I pass those two functions here and I call the execute function. And that's what we're going to see. When you call this, the first things come out of the model is the get current weather in the Paris and location. And this is function result. I should say function called function response, but better than make a function response. So my function, that executed help for function, executed the relevant functions, and got the dummy data, and then pass it to model, and now we have another function called from the model. Model asking for another function to be called is get current time, which is the look values of San Francisco, and then the dummy functions returns back to 10, and then we said the water in Paris in today is 10 degrees Celsius and current time in San Francisco is 10 because I just returned 10 That interesting So the model could really check each one of these questions separately It didn miss stuff between Paris and San Francisco And some weaker models, they do that. Perfect. And I tried, I think you'd be more than the two, three, four. You can continue doing that. That is really cool stuff. And then the second example is a little bit different. So I do have this message first. What's the word like today in Paris? So you can see that the second one is dependent on the first one. So there's a chain between these two, nested calls, binding, and there's dependency actually. So I pass the two functions, get current temperatures and convert temperature. And then let's see what's going to happen. So the first function called, the first function detected is the current temperature, is the Paris. So we execute the function, return it back to the model, and model it's rate again. And the second is the convert temperature, which is the 10 from the previous function called, it and then from Celsius to Fahrenheit and because it's a dummy function as it turns back at 10 always and we got this one perfect so this one Matt that that's really interesting very clean and then you can continue doing that to get the response and and you know by the way you can you can do some apply some smart trick to build your own a parallel function call I'll talk about it maybe someday even even the model doesn't support now Now let's go to the challenge that none of them could do it with the GPD. So I have sine, I have cosine, exponential in the power, and I'm passing this one. Calculate the result of power of sign of exponential 10 plus cosine of the power of two, exponent and then when you took it all, then there's another power, use this as a base and the four as exponent. Okay, so let's see what's going to happen. it here so we have a bunch of function called first of all fantastic that it didn't try to solve it by itself some of the models they do that so it took it really brick it down I like that first went for exponential 10 and that's what you got here then called the sign over the exponential 10 so that is great cool things then went for the power two three perfect then calculate the cosine of that nice eight and then got this and then got the power okay so what it did it did the addition by itself it did the addition between this part and this part let me see if the addition is correct minus this addition doesn't look correct it looks like it looks like subtraction i guess let me test it with my calculator uh is is 0.68 minus yeah it's Yeah, it's subtracted. Basically, this, if this minus this, it's going to give you the, but let me make sure everything is correct. Yeah. Well, okay, but that's beautiful. It got the exponent four, and this is great. This is fantastic. I mean, I think by maybe a little bit of adjustment, we can make it better. Or maybe we can create an extra function sum and don't let the model to do this edition by itself. We make it the sum like a binary operator, this sum of this, comma, this. You get what I mean? I will try that one. But that's fantastic. This absolutely, this is beautiful. This is great. Now I have, when I tried nostril, I said, I think so far this is the best function called model. But now I'm going to tell the same thing for this quote one. But seven to billion parameters, you know, that's the things. I like to have a function calling with 100 million parameters model. Anyway, this is great. This is a good model. I'm going to dig in more into this. And then that's it. I'll share the links to collapse so you can play around whether you can use that helper function. See how it goes. So this was Cohen 2. You can go to their blog. Definitely go to their blogs. A bunch of really interesting information over there. You can see what they have done. And that's it. So, Cohen 2, function calling. It was nice. And Uncle Code out.\",\"x_groq\":{\"id\":\"req_01j67j2k3bf4br65txs4fnzpf3\"}}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!hermes input.mp4 -p groq --response_format json"
      ],
      "id": "BUsIWsbkCIF3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvmhqNx1CIF3"
      },
      "source": [
        "### LLM Processing"
      ],
      "id": "QvmhqNx1CIF3"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z49ZHZLhCIF3",
        "outputId": "fc557396-3edf-4630-fe63-330d8803039a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Hello, your beautiful people. This is Uncle Code and today I'm going to review quickly the Q1-2 function calling ability. This model is really, to me, is very interesting. First of all, it came up with the really good stuff with 72 billion models and you can take a look, check their blog and especially their instruct model. And also it supports the majority of the language in the world, the regions, the major regions. And that is great because this is a large language model sounds like all of us and it's cool and go and play around with it so what I'm what I'm trying to do is trying to challenge a little bit the function calling like similar the thing that I did for other models like the mistral so first they have this nice library the coin agents that led you to create agentic software and applications and also speed up the work with the large language model what you can just install it quickly and then there are different ways that you can work with the model you can maybe use Olama or Lama CCP and then you download it and install it or I just choose open rotor because they do have this carbon to 72 billion in Strug model and I took it from re-increting account so you can have an API tokens is not difficult anyway I just the code is here from their own GitHub if you want to use by Ollama then then okay then if if you You know what? When you call this models, in a typical way is you call a model, you pass the user query, and then the model returns you back a message that it has function called key in it, and content is empty or is not. And then you take the name of the function and then go to your functions and pass the detected, extract your arguments, get the result back, send it back to the model, and then the model produced the response for you. This process, when you do have multiple calls, a user asking you questions that then when we get the first response then we have another call and again another call so this is going to be in kind of iteration and then you have to continue doing that so what if down I created a helper function which get the LM instance in this example for the coin and then messages in the list of functions you know JSON schema And it goes through them and then there a loop and if the last message of the response from the LLM has function called, and get it out, they call the functions, and create the function message and send it back to the message in each story, and then iterate, till the point that there's no any kind of function call and return it back. So it's a great helper because you can see, you're gonna see it, because one of the thing that I want to try is this multiple call back to each other's. So that's one thing. And also I created some dummy functions. Get current weather, just slip a little bit, and then return back in JSON in always Celsius. This is get current temperature, which is very similar to this one. Doesn't have the format. I just wanted to have multiple version of it. You will see later. And get current time, convert temperature. And I do have some mathematical functions, sine, cosine, exponential, and power. And they're really calculating. If you remember, if you have seen my video about Mistrol, there I challenge it with the complicated mathematical expressions. I wanted to see if you can break it down and then doing it. It was, couldn't, you know, but sort of half-half. Now let's see what's going to happen for this model. First of all, this model will return one functions. So it's not a multiple function, the parallel functions. So you have to iterate, like what I said. Okay, the first example. the better luck today in Paris also tell me the time in San Francisco. So there are two questions not necessarily relevant to each order. You can do this first or you can do the other one. But I want to see how it goes. And I pass those two functions here and I call the execute function. And that's what we're going to see. When you call this, the first things come out of the model is the get current weather in the Paris and location. And this is function result. I should say function called function response, but better than make a function response. So my function, that executed help for function, executed the relevant functions, and got the dummy data, and then pass it to model, and now we have another function called from the model. Model asking for another function to be called is get current time, which is the look values of San Francisco, and then the dummy functions returns back to 10, and then we said the water in Paris in today is 10 degrees Celsius and current time in San Francisco is 10 because I just returned 10 That interesting So the model could really check each one of these questions separately It didn miss stuff between Paris and San Francisco And some weaker models, they do that. Perfect. And I tried, I think you'd be more than the two, three, four. You can continue doing that. That is really cool stuff. And then the second example is a little bit different. So I do have this message first. What's the word like today in Paris? So you can see that the second one is dependent on the first one. So there's a chain between these two, nested calls, binding, and there's dependency actually. So I pass the two functions, get current temperatures and convert temperature. And then let's see what's going to happen. So the first function called, the first function detected is the current temperature, is the Paris. So we execute the function, return it back to the model, and model it's rate again. And the second is the convert temperature, which is the 10 from the previous function called, it and then from Celsius to Fahrenheit and because it's a dummy function as it turns back at 10 always and we got this one perfect so this one Matt that that's really interesting very clean and then you can continue doing that to get the response and and you know by the way you can you can do some apply some smart trick to build your own a parallel function call I'll talk about it maybe someday even even the model doesn't support now Now let's go to the challenge that none of them could do it with the GPD. So I have sine, I have cosine, exponential in the power, and I'm passing this one. Calculate the result of power of sign of exponential 10 plus cosine of the power of two, exponent and then when you took it all, then there's another power, use this as a base and the four as exponent. Okay, so let's see what's going to happen. it here so we have a bunch of function called first of all fantastic that it didn't try to solve it by itself some of the models they do that so it took it really brick it down I like that first went for exponential 10 and that's what you got here then called the sign over the exponential 10 so that is great cool things then went for the power two three perfect then calculate the cosine of that nice eight and then got this and then got the power okay so what it did it did the addition by itself it did the addition between this part and this part let me see if the addition is correct minus this addition doesn't look correct it looks like it looks like subtraction i guess let me test it with my calculator uh is is 0.68 minus yeah it's Yeah, it's subtracted. Basically, this, if this minus this, it's going to give you the, but let me make sure everything is correct. Yeah. Well, okay, but that's beautiful. It got the exponent four, and this is great. This is fantastic. I mean, I think by maybe a little bit of adjustment, we can make it better. Or maybe we can create an extra function sum and don't let the model to do this edition by itself. We make it the sum like a binary operator, this sum of this, comma, this. You get what I mean? I will try that one. But that's fantastic. This absolutely, this is beautiful. This is great. Now I have, when I tried nostril, I said, I think so far this is the best function called model. But now I'm going to tell the same thing for this quote one. But seven to billion parameters, you know, that's the things. I like to have a function calling with 100 million parameters model. Anyway, this is great. This is a good model. I'm going to dig in more into this. And then that's it. I'll share the links to collapse so you can play around whether you can use that helper function. See how it goes. So this was Cohen 2. You can go to their blog. Definitely go to their blogs. A bunch of really interesting information over there. You can see what they have done. And that's it. So, Cohen 2, function calling. It was nice. And Uncle Code out.\n",
            "\n",
            "LLM Processed Result:\n",
            "Here are three bullet points summarizing the transcription:\n",
            "\n",
            "• Uncle Code reviews the Q1-2 function calling ability of a large language model, specifically the 72 billion parameter model from the Cohere API. He finds that the model can process multiple function calls and return the results, which is a more complex task than simply executing a single function.\n",
            "\n",
            "• Uncle Code creates a helper function to speed up the process of function calling, which involves iterating through the model's responses and executing the relevant functions. He also creates dummy functions to test the model's ability to process multiple function calls, including mathematical functions like sine, cosine, and exponential.\n",
            "\n",
            "• The model is able to process complex function calls, including nested calls and dependencies, and returns the correct results. Uncle Code is impressed with the model's ability to break down complex mathematical expressions and execute them step-by-step, and he believes that this model is one of the best function calling models he has tested so far.\n"
          ]
        }
      ],
      "source": [
        "!hermes input.mp4 -p groq --llm_prompt \"Summarize this transcription in 3 bullet points\""
      ],
      "id": "Z49ZHZLhCIF3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-XcDMZZCIF3"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "That's it! You've learned the basics of using Hermes for lightning-fast video transcription. Explore the different providers, models, and response formats to find what works best for your needs. Happy transcribing!"
      ],
      "id": "e-XcDMZZCIF3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIKs3uffCIF4"
      },
      "source": [
        "**Extra Comments:**\n",
        "\n",
        "- Remember to replace the example video file paths and YouTube URLs with your actual content.\n",
        "- Hermes has excellent performance, especially with Groq's `distil-whisper` model.\n",
        "- Check out the `examples` folder in the GitHub repository for more advanced usage.\n",
        "- Feel free to contribute to the project and report any issues you encounter.\n",
        "- Don't forget to star the repo and follow [@unclecode](https://twitter.com/unclecode) on X!"
      ],
      "id": "gIKs3uffCIF4"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}